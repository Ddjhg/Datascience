<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Data Science Q&A</title>
    <style>
        body { font-family: Arial, sans-serif; line-height: 1.6; color: #333; max-width: 800px; margin: 0 auto; padding: 20px; }
        h1 { color: #2c3e50; }
        h2 { color: #3498db; }
        pre { background-color: #f4f4f4; padding: 10px; border-radius: 5px; overflow-x: auto; }
        .question { font-weight: bold; margin-top: 20px; }
        .answer { margin-bottom: 20px; }
    </style>
</head>
<body>
    <h1>Data Science Q&A</h1>

    <div class="question">
        <h2>1. How to deal with common data issues, contextual data issues, and data cleaning techniques?</h2>
    </div>
    <div class="answer">
        <p><strong>Common data issues:</strong></p>
        <ul>
            <li>Identify and handle missing values</li>
            <li>Detect and correct inconsistencies</li>
            <li>Remove duplicates</li>
            <li>Standardize data formats</li>
        </ul>
        <p><strong>Contextual data issues:</strong></p>
        <ul>
            <li>Ensure data relevance to the problem at hand</li>
            <li>Validate data against domain knowledge</li>
            <li>Check for temporal consistency</li>
            <li>Verify data completeness for the given context</li>
        </ul>
        <p><strong>Data cleaning techniques:</strong></p>
        <ul>
            <li>Data validation</li>
            <li>Data transformation</li>
            <li>Data normalization</li>
            <li>Outlier detection and handling</li>
            <li>Imputation of missing values</li>
        </ul>
    </div>

    <div class="question">
        <h2>2. How will you detect and diagnose the following data issues?</h2>
    </div>
    <div class="answer">
        <p><strong>(i) Missing values:</strong> Use descriptive statistics, visualizations (e.g., heatmaps), or built-in functions to identify null or empty values in the dataset.</p>
        <p><strong>(ii) Special values:</strong> Examine the data dictionary, look for unexpected or out-of-range values, and use domain knowledge to identify special codes or placeholders.</p>
        <p><strong>(iii) Outliers:</strong> Use statistical methods (e.g., z-score, IQR), visualizations (box plots, scatter plots), or machine learning techniques (isolation forests, LOF) to detect unusual data points.</p>
        <p><strong>(iv) Inconsistencies:</strong> Cross-validate related fields, check for logical contradictions, and use data profiling tools to identify inconsistent patterns or values.</p>
        <p><strong>(v) Localization:</strong> Examine date/time formats, currency symbols, and language-specific characters. Use regex or specialized libraries to detect and standardize localized data.</p>
    </div>

    <div class="question">
        <h2>3. How will you address advanced statistical issues?</h2>
    </div>
    <div class="answer">
        <p><strong>(i) Transformation:</strong> Apply mathematical functions (e.g., log, square root) to normalize data distribution, stabilize variance, or linearize relationships. Common transformations include Box-Cox, Yeo-Johnson, or custom functions based on data characteristics.</p>
        <p><strong>(ii) Deductive correction:</strong> Use logical rules, domain knowledge, or other variables to infer and correct erroneous values. This may involve creating decision trees or rule-based systems to identify and fix inconsistencies.</p>
        <p><strong>(iii) Deterministic imputation:</strong> Replace missing values using a predetermined, non-random method. Examples include mean/median imputation, last observation carried forward (LOCF), or using domain-specific rules to fill in gaps.</p>
    </div>

    <div class="question">
        <h2>4. Write k-mean clustering algorithm.</h2>
    </div>
    <div class="answer">
        <pre>
def k_means(data, k, max_iterations=100):
    # Randomly initialize k centroids
    centroids = random.sample(data, k)
    
    for _ in range(max_iterations):
        # Assign each point to the nearest centroid
        clusters = [[] for _ in range(k)]
        for point in data:
            distances = [euclidean_distance(point, centroid) for centroid in centroids]
            closest_centroid = distances.index(min(distances))
            clusters[closest_centroid].append(point)
        
        # Calculate new centroids
        new_centroids = []
        for cluster in clusters:
            if cluster:
                new_centroid = [sum(dim) / len(cluster) for dim in zip(*cluster)]
                new_centroids.append(new_centroid)
            else:
                new_centroids.append(random.choice(data))
        
        # Check for convergence
        if new_centroids == centroids:
            break
        
        centroids = new_centroids
    
    return centroids, clusters

def euclidean_distance(p1, p2):
    return sum((a - b) ** 2 for a, b in zip(p1, p2)) ** 0.5
        </pre>
    </div>

    <div class="question">
        <h2>5. Write k-medoid clustering algorithm.</h2>
    </div>
    <div class="answer">
        <pre>
def k_medoids(data, k, max_iterations=100):
    # Randomly select k medoids
    medoids = random.sample(data, k)
    
    for _ in range(max_iterations):
        # Assign each point to the nearest medoid
        clusters = [[] for _ in range(k)]
        for point in data:
            distances = [manhattan_distance(point, medoid) for medoid in medoids]
            closest_medoid = distances.index(min(distances))
            clusters[closest_medoid].append(point)
        
        # Find new medoids
        new_medoids = []
        for i, cluster in enumerate(clusters):
            if cluster:
                new_medoid = min(cluster, key=lambda x: sum(manhattan_distance(x, p) for p in cluster))
                new_medoids.append(new_medoid)
            else:
                new_medoids.append(medoids[i])
        
        # Check for convergence
        if new_medoids == medoids:
            break
        
        medoids = new_medoids
    
    return medoids, clusters

def manhattan_distance(p1, p2):
    return sum(abs(a - b) for a, b in zip(p1, p2))
        </pre>
    </div>

    <div class="question">
        <h2>6. Write fuzzy c-mean clustering algorithm.</h2>
    </div>
    <div class="answer">
        <pre>
import numpy as np

def fuzzy_c_means(data, c, m, max_iterations=100, epsilon=1e-5):
    n = len(data)
    # Initialize membership matrix
    U = np.random.rand(n, c)
    U = U / np.sum(U, axis=1)[:, np.newaxis]
    
    for _ in range(max_iterations):
        # Calculate cluster centers
        centers = np.dot(U.T ** m, data) / np.sum(U.T ** m, axis=1)[:, np.newaxis]
        
        # Update membership matrix
        distances = np.zeros((n, c))
        for i in range(c):
            distances[:, i] = np.sum((data - centers[i]) ** 2, axis=1)
        
        new_U = 1 / (distances ** (2 / (m - 1)))
        new_U = new_U / np.sum(new_U, axis=1)[:, np.newaxis]
        
        # Check for convergence
        if np.linalg.norm(new_U - U) < epsilon:
            break
        
        U = new_U
    
    return U, centers
        </pre>
    </div>

    <div class="question">
        <h2>7. Explain the following:</h2>
    </div>
    <div class="answer">
        <p><strong>(i) Cluster:</strong> A cluster is a group of data points that are more similar to each other than to data points in other clusters. Similarity is often defined using distance measures in the feature space.</p>
        <p><strong>(ii) Clustering:</strong> Clustering is the process of grouping similar data points together based on certain characteristics or features. It's an unsupervised learning technique used to discover inherent structures or patterns in data.</p>
        <p><strong>(iii) Hard clustering:</strong> In hard clustering, each data point belongs to exactly one cluster. The membership of a data point to a cluster is binary (0 or 1). K-means and K-medoids are examples of hard clustering algorithms.</p>
        <p><strong>(iv) Soft clustering:</strong> Soft clustering, also known as fuzzy clustering, allows data points to belong to multiple clusters with varying degrees of membership. Each data point has a probability or membership value for each cluster. Fuzzy C-means is an example of a soft clustering algorithm.</p>
    </div>

    <div class="question">
        <h2>8. Differentiate between intracluster and intercluster distances.</h2>
    </div>
    <div class="answer">
        <p><strong>Intracluster distance:</strong> This refers to the distance between data points within the same cluster. A small intracluster distance indicates that the points within a cluster are tightly grouped and similar to each other.</p>
        <p><strong>Intercluster distance:</strong> This is the distance between different clusters, typically measured as the distance between cluster centroids or the minimum distance between points from different clusters. A large intercluster distance suggests that the clusters are well-separated from each other.</p>
        <p>In an ideal clustering scenario, we aim to minimize intracluster distances while maximizing intercluster distances to achieve well-defined and distinct clusters.</p>
    </div>

    <div class="question">
        <h2>9. What is partition based clustering and how is it different from other types of clustering?</h2>
    </div>
    <div class="answer">
        <p>Partition-based clustering is a method that divides the data into a predetermined number of non-overlapping subsets or clusters. Key characteristics include:</p>
        <ul>
            <li>Requires specifying the number of clusters (k) in advance</li>
            <li>Iteratively refines the clusters</li>
            <li>Aims to optimize a specific criterion (e.g., minimizing within-cluster variance)</li>
            <li>Examples include K-means, K-medoids, and Fuzzy C-means</li>
        </ul>
        <p>Differences from other types of clustering:</p>
        <ul>
            <li>Hierarchical clustering: Builds a tree-like structure of clusters without requiring a pre-specified number of clusters</li>
            <li>Density-based clustering: Forms clusters based on areas of high data point density, can discover arbitrary shapes, and doesn't require specifying the number of clusters</li>
            <li>Model-based clustering: Assumes data comes from a mixture of probability distributions and uses statistical models to identify clusters</li>
        </ul>
    </div>

    <div class="question">
        <h2>10. Why soft clustering is preferred over hard clustering?</h2>
    </div>
    <div class="answer">
        <p>Soft clustering is often preferred over hard clustering for several reasons:</p>
        <ul>
            <li>Better representation of uncertainty: Soft clustering allows data points to have partial membership in multiple clusters, which can more accurately represent the inherent ambiguity in many real-world datasets.</li>
            <li>Handling overlapping clusters: In cases where clusters naturally overlap, soft clustering can capture this complexity better than hard clustering.</li>
            <li>Smoother optimization: The use of continuous membership values often leads to smoother optimization processes and can help avoid local optima.</li>
            <li>More informative results: Membership degrees provide additional information about the strength of association between data points and clusters.</li>
            <li>Better handling of outliers: Soft clustering can assign low membership values to outliers across multiple clusters, potentially reducing their impact on the overall clustering result.</li>
        </ul>
        <p>However, the choice between soft and hard clustering depends on the specific problem and dataset. Hard clustering may be preferred when clear, discrete categorizations are required or when the computational complexity of soft clustering becomes a limitation.</p>
    </div>

    <div class="question">
        <h2>11. Solve the following: K-medoid clustering using manhattan distance formula</h2>
    </div>
    <div class="answer">
        <p>Given points: (a,c), (3,4), (3,8), (4,7), (6,2), (6,4), (7,3), (7,4), (8,5), (7,6)</p>
        <p>Assume k=2</p>
        <p>Step 1: Initialize medoids</p>
        <p>Let's randomly choose two initial medoids: M1 = (3,4) and M2 = (7,4)</p>
        <p>Step 2: Assign points to nearest medoid</p>
        <p>Calculate Manhattan distances to each medoid and assign to the nearest:</p>
        <ul>
            <li>(a,c): Cannot calculate due to unknown values</li>
            <li>(3,4): Distance to M1 = 0, to M2 = 4. Assign to M1</li>
            <li>(3,8): Distance to M1 = 4, to M2 = 8. Assign to M1</li>
            <li>(4,7): Distance to M1 = 4, to M2 = 4. Assign to M1 (or M2, tie-break)</li>
            <li>(6,2): Distance to M1 = 5, to M2 = 5. Assign to M1 (or M2, tie-break)</li>
            <li>(6,4): Distance to M1 = 3, to M2 = 1. Assign to M2</li>
            <li>(7,3): Distance to M1 = 5, to M2 = 1. Assign to M2</li>
            <li>(7,4): Distance to M1 = 4, to M2 = 0. Assign to M2</li>
            <li>(8,5): Distance to M1 = 6, to M2 = 2. Assign to M2</li>
            <li>(7,6): Distance to M1 = 6, to M2 = 2. Assign to M2</li>
        </ul>
        <p>Step 3: Recalculate medoids</p>
        <p>For each cluster, find the point that minimizes the sum of distances to all other points in the cluster.</p>
        <p>Step 4: Repeat steps 2-3 until convergence or maximum iterations</p>
        <p>Note: This is the initial iteration. The process would continue, potentially changing the medoids and cluster assignments until convergence.</p>
    </div>
</body>
  </html>
